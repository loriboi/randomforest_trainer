# Machine Learning Framework â€“ Dataset Analysis & Automated Random Forest Experiments

A framework that **analyzes a dataset** (profiling, visualization, schema extraction) and **automatically trains a Random Forest model** with multiple trials and experiments.  
Designed to be modular so that dataset analysis, preprocessing, and model training can be run separately or together.

---

## ğŸ“¦ Project Structure

<pre> ``` . â”œâ”€â”€ dataset_analysis.py # Toolkit: profiling, charts, YAML schema, profile_dataset() â”œâ”€â”€ main.py # CLI entrypoint (run with --analyze to profile the dataset) â”œâ”€â”€ datasets/ # Default location for input datasets (ignored by git) â”‚ â””â”€â”€ .gitkeep â”œâ”€â”€ dataset_analisys/ # Default output folder for analysis results (ignored by git) â”‚ â””â”€â”€ .gitkeep â”œâ”€â”€ environment.yml # Conda environment definition â”œâ”€â”€ .github/workflows/ci.yml # GitHub Actions CI workflow â”œâ”€â”€ LICENSE # MIT License â””â”€â”€ README.md ``` </pre>


---

## âš™ï¸ Requirements

- [Conda](https://docs.conda.io/) (Anaconda or Miniconda)
- Python 3.10+

---

## ğŸ“¥ Installation (Conda)

```bash
# Create the environment
conda env create -f environment.yml

# Activate it
conda activate mlframework

# (Optional) verify versions
python -V
python -c "import pandas, numpy, matplotlib, yaml; print('OK')"
```

If you prefer manual setup:

bash

conda create -n mlframework python=3.10 -y
conda activate mlframework
pip install pandas numpy matplotlib pyyaml

â–¶ï¸ Usage
Place your CSV dataset inside the datasets/ folder, for example:
datasets/my_dataset.csv

1) Run without analysis (default: skipped)
bash

python main.py --dataset datasets/my_dataset.csv
2) Run with analysis
bash

python main.py --dataset datasets/my_dataset.csv --analyze
3) Specify the label column (example: labels)
bash

python main.py --dataset datasets/my_dataset.csv --analyze --label-column labels
4) Useful options
bash

# Top-k categories for frequency analysis
python main.py --dataset datasets/my_dataset.csv --analyze --freq-top-k 30

# Number of bins for numeric histograms
python main.py --dataset datasets/my_dataset.csv --analyze --numeric-bins 40

# Limit number of generated plots
python main.py --dataset datasets/my_dataset.csv --analyze --max-num-plots 10 --max-cat-plots 8

# Do not save HTML overview
python main.py --dataset datasets/my_dataset.csv --analyze --no-html
ğŸ“Š Output Structure
When analysis is enabled, results are saved in:

pgsql

dataset_analisys/
â””â”€â”€ my_dataset/
    â”œâ”€â”€ tables/
    â”‚   â”œâ”€â”€ column_types.csv
    â”‚   â”œâ”€â”€ numeric_stats.csv
    â”‚   â”œâ”€â”€ categorical_<col>.csv
    â”‚   â””â”€â”€ label_distribution.csv       # only if --label-column is valid
    â”œâ”€â”€ figures/
    â”‚   â”œâ”€â”€ label_balance.png             # only if --label-column is valid
    â”‚   â”œâ”€â”€ hist_<numcol>.png
    â”‚   â””â”€â”€ freq_<catcol>.png
    â”œâ”€â”€ schema.yaml
    â””â”€â”€ overview.html                     # if not disabled
ğŸ”¬ Next Steps
This framework will be extended to:

Automatically train Random Forest models on the dataset

Perform multiple trials with:

Different tree counts

Different subsets of features

Various preprocessing strategies

Compare results automatically

ğŸ§ª GitHub Actions CI
A CI workflow runs on every push and pull request:

Lints the code with ruff

Creates a sample dataset

Runs a â€œdry-runâ€ analysis in headless mode (no HTML overview)

ğŸ“œ License
MIT License â€“ you are free to use, modify, and distribute this code for any purpose.